# Data Modeling with Postgres.
### 1. Introduction.

A startup called Sparkify wants to analyze all the collected data on songs and user activity on their new music streaming app. The company is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

The goal of this project is to create a Postgres database with tables designed to optimize queries on song play analysis, creating a database schema and ETL pipeline for this analysis. 

### 2. Available Data.

##### 2.1. Song Dataset.

The first dataset resides on the **/data/song_data/** directory and it's composed of JSON format files that contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`

##### 2.2. Log Dataset.

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The data of this dataset resides on **/data/log_data/** directory and it's partitioned by year and month. For example, one filepath in this dataset would be `log_data/2018/11/2018-11-12-events.json`.

### 3. Model Structure.

The model defined with the previous data is a Star Model composed of the following tables:

| TABLE | TYPE | DESCRIPTION | FIELDS | PRIMARY KEY |
|----------|-----------|-------------------------------------------------------------------|------------------------------------------------------------------------------------------------|-------------|
| songplay | Fact | Records in log data associated with song plays | songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent  | songplay_id |
| user | Dimension | Users in the app | user_id, first_name, last_name, gender, level | user_id |
| song | Dimension | Songs in music database | song_id, title, artist_id, year, duration | song_id |
| artist | Dimension | Artists in music database | artist_id, name, location, latitude, longitude | artist_id |
| time | Dimension | Timestamps of records in songplays broken down into specific unit | start_time, hour, day, week, month, year, weekday | start_time |


### 4. Project Structure.

On this section we explain the different sections of the project. The order in which they are discussed it's the execution order of the different scripts.

##### 4.1. `sql_queries.py`.

On this part we define all the sql queries invoked by each module of the code. For example, the syntaxis to drop tables of the database, create the metadata information for each table in Postgress and the inserts queries of each table. ***It's necessary to have this file completed before executing any other script.***

##### 4.2. `create_tables.py`.

This script create Sparkify database, connects to it and drops and create the tables defined on the previous section.

##### (Optional): Jupyter Notebook ETL.

This module has been coded to check etl commands and steps that will be productivized in `etl.py` script (next step). There, every step can be checked and analyzed.
##### 4.3. `etl.py`.

This module has almost all the logic of the processed. It reads the JSON files of songs and logs to create each of the tables, processed the different files to create every single table and insert them on its corresponding Postgres table.

##### 4.4. `test.ipynb`. 

It is a Jupyter notebook to check if the insetion of the previous script has been properly made.
 
